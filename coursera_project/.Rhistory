?':'
sseq(1,20)
seq(1,20)
seq(0,10, by=0.5)
seq(5,10, length=30)
my_seq<-seq(5,10, length=30)
length(my_seq)
1:length(my_seq)
seq(along.with = my_seq)
seq_along(my_seq)
rep(0,times=40)
rep(c(0,1,2), times=10)
rep(c(0,1,2),each=10)
num_vect <- c(0.5, 55, -10, 6)
tf <- num_vect<1
tf
num_vect >= 6
my_char <- c("My", "name", "is")
my_char
paste(my_char, collapse = " ")
my_name <- c(my_char, "Himanshu")
my_name
paste(my_name, collapse = " ")
paste("Hello" , "Paste" , sep =" ")
paste("Hello" , "world!" , sep =" ")
paste(1:3, c("X", "Y", "Z"), sep="")
paste(LETTERS, 1:4, sep = "-")
x <- c(44, NA, 5, NA)
x*3
y <- rnorm(1000)
z<- rep(NA, 1000)
my_data <- sample(c(y,z), 100)
my_na <- is.na(my_data)
my_na
my_data == NA
sum(my_na)
my_data
0/0
Inf-Inf
x
x[1:10]
x[is.na(x)]
y<- x[!is.na(x)]
y
y[y>0]
x[x>0]
x[!is.na(x) & x>0]
x[c(3,5,7)]
x[0]
x[3000]
x[c(-2,-10)]
x[-c(2,10)]
vect<- c(foo=11, bar  =2, norf =NA)
vect
names(vect)
vect2<- c(11,2,NA)
names(vect2)<-c("foo", "bar", "norf")
identical(vect, vect2)
vect["bar"]
vect[c("foo", "bar")]
library(XML)
theurl <- "http://biostat.jhsph.edu/~jleek/contact.html"
tables <- readHTMLTable(theurl)
n.rows <- unlist(lapply(tables, function(t) dim(t)[1]))
nchar(n.rows[10])
n.rows[10]
tables[10]
library(RCurl)
library(XML)
# Download page using RCurl
# You may need to set proxy details, etc.,  in the call to getURL
theurl <- "http://biostat.jhsph.edu/~jleek/contact.html"
webpage <- getURL(theurl)
# Process escape characters
webpage <- readLines(tc <- textConnection(webpage)); close(tc)
# Parse the html tree, ignoring errors on the page
pagetree <- htmlTreeParse(webpage, error=function(...){})
# Navigate your way through the tree. It may be possible to do this more efficiently using getNodeSet
body <- pagetree$children$html$children$body
divbodyContent <- body$children$div$children[[1]]$children$div$children[[4]]
tables <- divbodyContent$children[names(divbodyContent)=="table"]
#In this case, the required table is the only one with class "wikitable sortable"
tableclasses <- sapply(tables, function(x) x$attributes["class"])
thetable  <- tables[which(tableclasses=="wikitable sortable")]$table
#Get columns headers
headers <- thetable$children[[1]]$children
columnnames <- unname(sapply(headers, function(x) x$children$text$value))
# Get rows from table
content <- c()
for(i in 2:length(thetable$children))
{
tablerow <- thetable$children[[i]]$children
opponent <- tablerow[[1]]$children[[2]]$children$text$value
others <- unname(sapply(tablerow[-1], function(x) x$children$text$value))
content <- rbind(content, c(opponent, others))
}
# Convert to data frame
colnames(content) <- columnnames
as.data.frame(content)
content
library(XML)
doc.html = htmlTreeParse('http://biostat.jhsph.edu/~jleek/contact.html',useInternal = TRUE)
# Extract all the paragraphs (HTML tag is p, starting at
# the root of the document). Unlist flattens the list to
# create a character vector.
doc.text = unlist(xpathApply(doc.html, '//p', xmlValue))
head(doc.text)
nchar(doc.text[10])
doc.text[10]
library(XML)
library(XML)
doc.html = htmlTreeParse('http://biostat.jhsph.edu/~jleek/contact.html',useInternal = TRUE)
# Extract all the paragraphs (HTML tag is p, starting at
# the root of the document). Unlist flattens the list to
# create a character vector.
doc.text = unlist(xpathApply(doc.html, '//p', xmlValue))
doc.text = gsub('\\n', ' ', doc.text)
nchar(doc.text[10])
library(XML)
doc.html = htmlTreeParse('http://biostat.jhsph.edu/~jleek/contact.html',useInternal = TRUE)
# Extract all the paragraphs (HTML tag is p, starting at
# the root of the document). Unlist flattens the list to
# create a character vector.
doc.text = unlist(xpathApply(doc.html,  xmlValue))
head(doc.text)
tail(doc.text)
library(XML)
u <- "http://biostat.jhsph.edu/~jleek/contact.html"
tables <- readHTMLTable(u)
library(XML)
u <- "http://biostat.jhsph.edu/~jleek/contact.html"
tables <- readHTMLTable(u)
library(XML)
u <- "http://biostat.jhsph.edu/~jleek/contact.html"
tables <- readHTMLTable(u)
tables[10]
tables[[10]]
library(XML)
u <- "http://biostat.jhsph.edu/~jleek/contact.html"
q <- xmlParse(u)
q[10]
library(XML)
u <- "http://biostat.jhsph.edu/~jleek/contact.html"
q <- xmlParse(u,isHTML = TRUE)
library(XML)
u <- "http://biostat.jhsph.edu/~jleek/contact.html"
q <- xmlParse(u,isHTML = TRUE);
library(XML)
u <- "http://biostat.jhsph.edu/~jleek/contact.html"
q <- htmlParse(u)
head(q)
class(q)
web_page <- readLines("http://biostat.jhsph.edu/~jleek/contact.html")
web_page[10]
nchar(web_page[10])
nchar(web_page[c(10,20,30,100])
nchar(web_page[c(10,20,30,100)]
web_page <- readLines("http://biostat.jhsph.edu/~jleek/contact.html")
nchar(web_page[c(10,20,30,100)]
web_page <- readLines("http://biostat.jhsph.edu/~jleek/contact.html")
nchar(web_page[c(10,20,30,100)])
a1<- readLines("https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for")
head(a1[,4])
a1[,4]
head(a1)
a1[4:length(a1)]
x <- read.fwf(
file=url("https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"),
skip=4,
widths=c(12, 7,4, 9,4, 9,4, 9,4))
head(x)
sum(x[,4])
library(swirl)
install_from_swirl("Getting and Cleaning Data")
swirl()
set.seed(1)
rpois(5,2)
install.packages("shiny")
library(shiny)
library(shiny)
install.packages(c("colorspace", "manipulate", "Rcpp", "RMySQL"))
library(shiny)
library(shiny)
shiny::runApp('DSP and tools/R/Shiny R')
shiny::runApp('DSP and tools/R/Shiny R')
shiny::runApp('DSP and tools/R/Shiny R')
shiny::runApp('DSP and tools/R/Shiny R')
shiny::runApp('DSP and tools/R/Shiny R')
library(swirl)
swirl()
5+7
x<-5+7
x
y<-x-3
y
z<-c(1.1,9,3.14)
?c()
?c
z
c(z,555,z)
z*2+100
my_sqrt<-z-1
my_sqrt<-sqrt(z-1)
my_sqrt
my_div<-z/my_sqrt
my_div
c(1,2,3,4)+c(0,10)
c(1, 2, 3, 4) + c(0, 10, 100)
z*2+1000
my_sqrt
my_div
getwd()
ls()
x<-9
ls()
list.files()
?list.files
args(list.files)
old.dir<-"C:/Users/Himanshu/Documents"
old.dir<-getwd()
makedir("testdir")
mkdir("testdir")
?createdir
??createdir
?mkdir
?dir
dir("testdir")
dir.create("testdir")
setwd(testdir)
getwd()
setwd("testdir")
file.create("mytest.R")
list.files()
ls()
file.exists("mytest.R")
info("mytest.R")
args("mytest.R")
list.files("mytest.R")
file.info("mytest.R")
file.rename("mytest.R", "mytest2.R")
file.copy("mytest2.R", "mytest3.R")
file.path("mytest3.R")
?dir.create
dir.create("testdir2", subdir.create("testdir3"))
dir.create("testdir2", dir.create("testdir3"))
dir.create("testdir2/testdir3", recursive=TRUE)
dir.delete("testdir2")
dir.remove("testdir2")
?dir.remove
?dir.delete?
h
?dir.delete
unlink("testdir2", recursive=TRUE)
old.dir()
setwd("old.dir")
old.dir
setwd(old.dir)
unlink("testdir", recursive = TRUE)
1:20
pi:10
15:1
?:
:
?':'
seq(1,20)
seq(0,10, by=0.5)
my_seq<-seq(5,10, length=30)
length()
length(my_seq)
1:length(my_seq)
seq(along.with = my_seq)
seq_along(my_seq)
rep(0, times=40)
rep(c(0,1,2), times = 10)
rep(c(0,1,2), each=10)
num_vect<-c(0.5,55,-10,6)
tf<-num_vect<1
tf
num_vect>=6
play()
111>111
111>=111
nxt()
my_char<-(c("My", "name" ,"is"))
my_char<-c("My", "name" ,"is")
my_char
paste(my_char, collapse=" ")
my_name<-c(my_char, "Nisha")
my_name
paste(my_name, collapse=" ")
paste("Hello", "world!", sep=" ")
paste(1:3, c("X", "Y", "Z"), sep="")
paste(LETTERS, 1:4, sep = "-")
x<-c(44, NA, 5, NA)
x*3
y <- rnorm(1000)
z <- rep(NA, 1000)
my_data <- sample(c(y, z), 100)
my_na<-is.na(my_data)
my_na
my_data==NA
summy_na
sum(my_na) && sum(my_data)
sum(my_na)
my_data
0/0
Inf/Inf
Inf-Inf
x
x[1:10]
x[is.na(x)]
y<-x[!is.na(x)]
y
y[y > 0]
x[x > 0]
x[!is.na(x) & x > 0]
x[3,5,7]
x[3]
x[c(3,5,7)]
x[0]
x[3000]
x[c(-2, -10)]
x[-c(2, 10)]
vect <- c(foo = 11, bar = 2, norf =NA)
vect
names(vect)
vect2<-c(11,2,NA)
names(vect2)<-c("foo", "bar", "norf")
identical(vect,vect2)
vect["bar"]
vect[c("foo", "bar")]
my_vector<-1:20
my_vector
dim(my_vector)
length(my_vector)
dim(my_vector)<-c(4,5)
dim(my_vector)
attributes(my_vector)
my_vector
class(my_v)
class(my_vector)
my_matrix<-my_vector
?matrix()
?matrix
my_matrix<-(1:20,4,5)
my_matrix<-c(1:20),4,5)
my_matrix
my_matrix<- matrix(1:20,4,5)
my_matrix2<- matrix(1:20,4,5)
identical(my_matrix, my_matrix2)
patients<-c("Bill", "Gina", "Kelly", "Sean")
cbind(patients, my_matrix)
my_data <- data.frame(patients, my_matrix)
my_data
class(my_data)
cnames("patient", "age",  "weight", "bp", "rating", "test")
cnames<-c("patient", "age",  "weight", "bp", "rating", "test")
colnames(my_data)
colnames(my_data)<-cnames
my_data
TRUE==TRUE
(FALSE == TRUE) == FALSE
6==7
6<7
10<=10
5!=7
!5=7
!(5==7)
FALSE & FALSE
TRUE & c(TRUE, FALSE, FALSE)
TRUE && c(TRUE, FALSE, FALSE)
TRUE | c(TRUE, | FALSE, FALSE)
TRUE | c(TRUE, FALSE, FALSE)
TRUE || c(TRUE, FALSE, FALSE)
5 > 8 || 6 != 8 && 4 > 3.9
isTRUE(6<4)
isTRUE(6>4)
identical('twins', 'twins')
xor(5 == 6, !FALSE)
ints
ints<- sample(10)
ints
ints>5
which(ints>5)
which(ints>7)
any(ints<0)
all(ints>0)
head(flags)
dim(flags)
class(flags)
cls_list <- lapply(flags, class)
cls_list
class(cls_list)
as.character(cls_list)
cls_vect<- sapply(flags, class)
cls_vect
class(cls_vect)
sum(flags$orange)
flag_colors <- flags[,11:17]
head(flag_colors)
lapply(flag_colors, sum)
sapply(flag_colors, sum)
sapply(flag_colors, mean)
swirl()
flag_shapes <- flags[, 19:23]
lapply(flag_shapes, range)
shape_mat<- sapply(flag_shapes, range)
shape_mat
class(shape_mat)
unique(c(3, 4, 5, 5, 5, 6, 6))
unique_vals <- lapply(flags, unique)
unique_vals
sapply(unique_vals, length)
sapply(flag, unique)
sapply(flags, unique)
lapply(unique_vals, function(elem) elem[2])
x <- read.csv(file = "C:\\Users\\Himanshu\\Nisha\\match\\Match.csv", header=TRUE)
link <- as.vector(x$search)
y <- NULL
for (i in link)
{
y[i] <- paste("https://www.google.co.in/search?q=",i,sep = "")
}
class(y)
y <- as.vector(y)
y[1]
x <- read.csv(file = "C:\\Users\\Himanshu\\Nisha\\match\\Match.csv", header=TRUE)
link <- as.vector(x$search)
y <- NULL
for (i in 1:length(link))
{
y[i] <- paste("https://www.google.co.in/search?q=",link[i],sep = "")
y[i] <- gsub(pattern = " ", replacement = "%20",x = y[i])
}
class(y)
y <- as.vector(y)
y[1]
library(RCurl)
library(XML)
library(httr)
z <- getURL(url =y[1, ], .opts = list(ssl.verifypeer = FALSE))
z <- getURL(url =y[1], .opts = list(ssl.verifypeer = FALSE))
z <- getURL(url =y[1], .opts = list(ssl.verifypeer = FALSE))
z
zParse <- xmlTreeParse(z, useInternal = TRUE)
zParse <- xmlTreeParse(z, useInternalNodes = TRUE)
zParse <- htmlTreeParse(z, useInternalNodes = TRUE)
zParse
zroot <- xmlRoot(zParse)
zroot
xmlName(zroot)
library(curl)
installed.packages("curl")
install.packages("curl")
swirl()
library(swirl)
swirl()
library(tidyr)
students\
students
students
student
students
library(httr)
oauth_endpoints("github")
myapp <- oauth_app("prayogshala",
key = "4c2135ead54263895257",
secret = "0191da7055c035625606d97e03fde2d932575a6c")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
oauth_endpoints("github")
myapp <- oauth_app("prayogshala",
key = "4c2135ead54263895257",
secret = "0191da7055c035625606d97e03fde2d932575a6c")
# 3. Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
oauth_endpoints("github")
# 2. To make your own application, register at at
#    https://github.com/settings/applications. Use any URL for the homepage URL
#    (http://github.com is fine) and  http://localhost:1410 as the callback url
#
#    Replace your key and secret below.
myapp <- oauth_app("prayogshala",
key = "4c2135ead54263895257",
secret = "0191da7055c035625606d97e03fde2d932575a6c")
# 3. Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
stop_for_status(req)
content(req)
library(httr)
oauth_endpoints("github")
myapp <- oauth_app("prayogshala",
key = "4c2135ead54263895257",
secret = "0191da7055c035625606d97e03fde2d932575a6c")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
stop_for_status(req)
library(dplyr)
select(req, contains("date"))
class(req)
dim(req)
req1 <- as.list(req)
req1
req2 <- as.data.frame(req)
setwd("C:\\Users\\Himanshu\\Documents\\DSP and tools\\Coursera classes\\Geeting and Cleaning data\\coursera_project")
install.packages("downloader")
library(downloader)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"
download(url, dest="dataset.zip", mode="wb")
unzip ("dataset.zip")
